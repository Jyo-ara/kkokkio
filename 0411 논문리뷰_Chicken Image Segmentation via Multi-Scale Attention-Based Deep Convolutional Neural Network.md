# Title

![image](https://github.com/user-attachments/assets/4995e32a-deed-4241-8bd6-5d2b42bb36ff)

---
---

# ABSTRACT ì´ˆë¡

**Accurate segmentation and analysis for each animal in surveillance video images will help poultry farmers to monitor and promote animal welfare.**  
ê°ì‹œ ì˜ìƒ ì† ê° ë™ë¬¼ì˜ ì •í™•í•œ ë¶„í•  ë° ë¶„ì„ì€ ì–‘ê³„ ë†ê°€ê°€ ë™ë¬¼ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë³µì§€ë¥¼ ì¦ì§„í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

**However, it is challenging to accurately segment each animal due to the similar appearance, different scales, rapid growth and adhesive areas of group animals.**  
í•˜ì§€ë§Œ ë¬´ë¦¬ì§€ì–´ ìˆëŠ” ë™ë¬¼ë“¤ì˜ ìœ ì‚¬í•œ ì™¸í˜•, ë‹¤ì–‘í•œ í¬ê¸°, ë¹ ë¥¸ ì„±ì¥, ê·¸ë¦¬ê³  ì„œë¡œ ë¶™ì–´ ìˆëŠ” ì˜ì—­ ë•Œë¬¸ì— ê°œë³„ ë™ë¬¼ì„ ì •í™•í•˜ê²Œ ë¶„í• í•˜ëŠ” ê²ƒì€ ì–´ë µìŠµë‹ˆë‹¤.

**Meanwhile, lacking of useful training data also limits the effectiveness of animal segmentation algorithms.**  
í•œí¸, ìœ ìš©í•œ í•™ìŠµ ë°ì´í„°ì˜ ë¶€ì¡±ë„ ë™ë¬¼ ë¶„í•  ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ì œí•œí•©ë‹ˆë‹¤.

**To address these problems, we first construct a chicken image segmentation dataset to study the behavior of chickens for intelligent monitoring and analysis.**  
ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ë¨¼ì € ì§€ëŠ¥í˜• ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„ì„ ìœ„í•œ ë‹­ì˜ í–‰ë™ì„ ì—°êµ¬í•  ìˆ˜ ìˆëŠ” ë‹­ ì´ë¯¸ì§€ ë¶„í•  ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

**Then, we propose an effective end-to-end framework for chicken image segmentation, which can also be used for other animal image segmentation.**  
ê·¸ ë‹¤ìŒ, ë‹­ ì´ë¯¸ì§€ ë¶„í• ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ì—”ë“œ-íˆ¬-ì—”ë“œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•˜ë©°, ì´ëŠ” ë‹¤ë¥¸ ë™ë¬¼ì˜ ì´ë¯¸ì§€ ë¶„í• ì—ë„ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**An end-to-end multi-scale based encoder-decoder network is first utilized to extract multi-scale features.**  
ìš°ì„ , ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ê¸°ë°˜ì˜ ì—”ë“œ-íˆ¬-ì—”ë“œ ì¸ì½”ë”-ë””ì½”ë” ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ìš©í•´ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

**Then, an attention-based module is employed to extract and intensify effective features, thus better segmentation results can be obtained.**  
ê·¸ ë‹¤ìŒ, ì–´í…ì…˜(attention) ê¸°ë°˜ ëª¨ë“ˆì„ ì‚¬ìš©í•´ ìœ íš¨í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ê°•ì¡°í•¨ìœ¼ë¡œì¨, ë” ë‚˜ì€ ë¶„í•  ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Finally, a multi-output combined loss function is proposed to make effective supervision for better segmentation.**  
ë§ˆì§€ë§‰ìœ¼ë¡œ, ë” ë‚˜ì€ ë¶„í• ì„ ìœ„í•œ íš¨ê³¼ì ì¸ í•™ìŠµ ì§€ë„ë¥¼ ìœ„í•´ ë‹¤ì¤‘ ì¶œë ¥ ê²°í•© ì†ì‹¤ í•¨ìˆ˜ê°€ ì œì•ˆë©ë‹ˆë‹¤.

**Experimental results demonstrate the promising performance of the proposed framework for chicken image segmentation.**  
ì‹¤í—˜ ê²°ê³¼ëŠ” ì œì•ˆëœ í”„ë ˆì„ì›Œí¬ê°€ ë‹­ ì´ë¯¸ì§€ ë¶„í• ì—ì„œ ìœ ë§í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

![image](https://github.com/user-attachments/assets/65d90ce9-225a-43be-9589-91d2a7a89265)

---
---

# I. INTRODUCTION ì„œë¡ 

**Recently, many studies focus on observing and analyzing the behaviour of animal to prevent diseases for animal welfare [1].**  
ìµœê·¼ ë™ë¬¼ ë³µì§€ë¥¼ ìœ„í•œ ì§ˆë³‘ ì˜ˆë°©ì„ ëª©ì ìœ¼ë¡œ ë™ë¬¼ì˜ í–‰ë™ì„ ê´€ì°°í•˜ê³  ë¶„ì„í•˜ëŠ” ì—°êµ¬ê°€ í™œë°œíˆ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤ [1].

**The rapid development of artificial intelligence and image processing technology accelerates the process of automatic and intelligent farming.**  
ì¸ê³µì§€ëŠ¥ê³¼ ì˜ìƒì²˜ë¦¬ ê¸°ìˆ ì˜ ë¹ ë¥¸ ë°œì „ì€ ìë™í™”ë˜ê³  ì§€ëŠ¥ì ì¸ ë†ì—…ì˜ ë°œì „ì„ ê°€ì†í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.

**Observing and analyzing animal automatically can be achieved by sensor-based methods [2], [3] and computer vision based methods [4]â€“[6].**  
ë™ë¬¼ì„ ìë™ìœ¼ë¡œ ê´€ì°°í•˜ê³  ë¶„ì„í•˜ëŠ” ë°©ë²•ì€ ì„¼ì„œ ê¸°ë°˜ ë°©ì‹ [2], [3]ê³¼ ì»´í“¨í„° ë¹„ì „ ê¸°ë°˜ ë°©ì‹ [4]â€“[6]ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**For the former, specific sensor devices are used to obtain information of position, number, etc.**  
ì „ìì˜ ê²½ìš°, íŠ¹ì • ì„¼ì„œ ì¥ë¹„ë¥¼ í†µí•´ ìœ„ì¹˜, ìˆ˜ëŸ‰ ë“±ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

**For examples, classical ear tags or collars can be exploited to locate positions of animals, however, it is very expensive and time-consuming to wear sensors for each animal.**  
ì˜ˆë¥¼ ë“¤ì–´, ì „í†µì ì¸ ê·€í‘œë‚˜ ëª©ê±¸ì´ ì„¼ì„œë¥¼ ì´ìš©í•´ ë™ë¬¼ì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìœ¼ë‚˜, ëª¨ë“  ê°œì²´ì— ì¥ì°©í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„ìš©ì´ ë§ì´ ë“¤ê³  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.

**For the latter, video or image data of animals need to be captured by cameras firstly, and then these data are processed by computer vision based methods for intelligent farming.**  
í›„ìì˜ ê²½ìš°, ì¹´ë©”ë¼ë¡œ ë™ë¬¼ì˜ ì˜ìƒì´ë‚˜ ì´ë¯¸ì§€ë¥¼ ì´¬ì˜í•œ í›„ ì»´í“¨í„° ë¹„ì „ ê¸°ë°˜ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì§€ëŠ¥í˜• ë†ì—…ì— í™œìš©í•©ë‹ˆë‹¤.

**The advantages of low price, easy installation and non-invasion make computer vision based methods more preferentially to monitor animal behavior [3].**  
ì»´í“¨í„° ë¹„ì „ ê¸°ë°˜ ë°©ì‹ì€ ì €ë ´í•œ ê°€ê²©, ì‰¬ìš´ ì„¤ì¹˜, ë¹„ì¹¨ìŠµì„± ë“±ì˜ ì¥ì ìœ¼ë¡œ ì¸í•´ ë™ë¬¼ í–‰ë™ ëª¨ë‹ˆí„°ë§ì— ë” ì„ í˜¸ë©ë‹ˆë‹¤ [3].

**Hence, in this paper, we mainly focus on computer vision based techniques for intelligent farming.**  
ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì»´í“¨í„° ë¹„ì „ ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ë†ì—… ê¸°ìˆ ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.

---

**Image segmentation, as the basic and important step to monitor animal health, has drawn lots of attentions in intelligent farming [3], [4], [7].**  
ì´ë¯¸ì§€ ë¶„í• ì€ ë™ë¬¼ ê±´ê°•ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•œ ê¸°ë³¸ì ì´ë©´ì„œ ì¤‘ìš”í•œ ê³¼ì •ìœ¼ë¡œ, ì§€ëŠ¥í˜• ë†ì—…ì—ì„œ ë§ì€ ê´€ì‹¬ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤ [3], [4], [7].

**Significant progress has been made to achieve more accurate segmentation results with the supervised framework of machine learning.**  
ë¨¸ì‹ ëŸ¬ë‹ì˜ ì§€ë„í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ë³´ë‹¤ ì •í™•í•œ ë¶„í•  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë° ìˆì–´ í° ì§„ì „ì´ ìˆì—ˆìŠµë‹ˆë‹¤.

**Particularly, deep convolutional neural networks (CNN) based methods [5], [6], [8] have achieved the state-of-the-art performance for image segmentation.**  
íŠ¹íˆ, ë”¥ í•©ì„±ê³± ì‹ ê²½ë§(CNN) ê¸°ë°˜ ë°©ë²• [5], [6], [8]ì€ ì´ë¯¸ì§€ ë¶„í•  ë¶„ì•¼ì—ì„œ ìµœì‹ ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.

**Accurate segmentation and analysis will help people to monitor and reveal deviations of animal, and to prevent losses in animal production process.**  
ì •í™•í•œ ë¶„í• ê³¼ ë¶„ì„ì€ ë™ë¬¼ì˜ ì´ìƒ í–‰ë™ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì¡°ê¸°ì— ë°œê²¬í•˜ì—¬ ìƒì‚° ê³¼ì •ì˜ ì†ì‹¤ì„ ì˜ˆë°©í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

---

**In this paper, we focus on studying image segmentation of chicken due to its large market demand.**  
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì‹œì¥ ìˆ˜ìš”ê°€ í° ë‹­ì˜ ì´ë¯¸ì§€ ë¶„í• ì— ì¤‘ì ì„ ë‘ê³  ì—°êµ¬í•©ë‹ˆë‹¤.

**Besides, the proposed segmentation framework can also be applied to other animals or objects segmentation.**  
ë˜í•œ ì œì•ˆëœ ë¶„í•  í”„ë ˆì„ì›Œí¬ëŠ” ë‹¤ë¥¸ ë™ë¬¼ì´ë‚˜ ê°ì²´ì˜ ë¶„í• ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Individual chicken segmentation is the basic step to segregate from group-level treatment and conduct individual chicken care.**  
ê°œë³„ ë‹­ ë¶„í• ì€ êµ°ì§‘ ë‹¨ìœ„ ì²˜ë¦¬ì—ì„œ ë²—ì–´ë‚˜ ê°œë³„ ë‹­ ê´€ë¦¬ë¡œ ë‚˜ì•„ê°€ê¸° ìœ„í•œ ê¸°ë³¸ ë‹¨ê³„ì…ë‹ˆë‹¤.

**Based on the segmented individual chicken, important information such as chicken position and chicken number can be obtained.**  
ë¶„í• ëœ ê°œë³„ ë‹­ì„ ë°”íƒ•ìœ¼ë¡œ ë‹­ì˜ ìœ„ì¹˜ì™€ ìˆ˜ëŸ‰ ë“± ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Thus, we can monitor chicken behaviour constantly and regard these as indicators of health and well-being.**  
ì´ë¥¼ í†µí•´ ë‹­ì˜ í–‰ë™ì„ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ê±´ê°•ê³¼ ë³µì§€ì˜ ì§€í‘œë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

**As observed, most chickens usually crowd together in a cage as shown in Fig. 1, it is challenging to monitor the behavior of group chickens by separating adhesive regions.**  
ê´€ì°°ëœ ë°”ì™€ ê°™ì´ ëŒ€ë¶€ë¶„ì˜ ë‹­ë“¤ì€ Fig. 1ì—ì„œì²˜ëŸ¼ ìš°ë¦¬ ì•ˆì— ë°€ì§‘ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ì„œë¡œ ë°€ì°©ëœ ì˜ì—­ì„ ë¶„ë¦¬í•´ ë¬´ë¦¬ í–‰ë™ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤.  
<p align="center">
    <img src="https://github.com/user-attachments/assets/42529c23-047d-4f0e-b516-c5889dd43ca1" alt="image" width="500">
</p>

**Besides, similar appearance and various scale, motion blur by chicken movement and object occlusions as shown in Fig. 2, which make it challenging for accurate chicken segmentation.**  
ë˜í•œ Fig. 2ì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼, ë‹­ë“¤ì˜ ìœ ì‚¬í•œ ì™¸í˜•, ë‹¤ì–‘í•œ í¬ê¸°, ì›€ì§ì„ìœ¼ë¡œ ì¸í•œ ë¸”ëŸ¬, ê°ì²´ ê°€ë¦¼ í˜„ìƒ ë“±ì´ ì •í™•í•œ ë¶„í• ì„ ì–´ë µê²Œ ë§Œë“­ë‹ˆë‹¤.  
<p align="center">
    <img src="https://github.com/user-attachments/assets/eb6058ad-f107-43c1-bbc6-5b7f5f2add28" alt="image" width="500">
</p>

> - Occlusion : ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ì–´ë–¤ ë¬¼ì²´ ì•ì— ì¥ì• ë¬¼ì´ ìˆì–´ì„œ ê°€ë ¤ì§€ëŠ” í˜„ìƒ  
> - Adhesion : ë¶€ì°©

---

**To address these issues mentioned above, we propose a multi-scale attention based deep network, which is named MSAnet, for effective chicken image segmentation.**  
ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” íš¨ê³¼ì ì¸ ë‹­ ì´ë¯¸ì§€ ë¶„í• ì„ ìœ„í•œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì–´í…ì…˜ ê¸°ë°˜ ë”¥ ë„¤íŠ¸ì›Œí¬ì¸ **MSAnet**ì„ ì œì•ˆí•©ë‹ˆë‹¤.

---

**The major contributions of this work include the followings.**  
**ë³¸ ì—°êµ¬ì˜ ì£¼ìš” ê¸°ì—¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.**

- **We construct a novel chicken dataset, which is captured by a monocular camera covered with different periods for chicken segmentation and behavior analysis.**  
  ë‹­ ì´ë¯¸ì§€ ë¶„í•  ë° í–‰ë™ ë¶„ì„ì„ ìœ„í•´, ë‹¤ì–‘í•œ ê¸°ê°„ì— ê±¸ì³ ë‹¨ì•ˆ ì¹´ë©”ë¼ë¡œ ì´¬ì˜í•œ ìƒˆë¡œìš´ ë‹­ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

- **To the best of our knowledge, we are the first one to construct the chicken dataset for the research of chicken image segmentation and analysis.**  
  ìš°ë¦¬ê°€ ì•„ëŠ” í•œ, ë‹­ ì´ë¯¸ì§€ ë¶„í•  ë° ë¶„ì„ì„ ìœ„í•œ ë‹­ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•œ ê²ƒì€ ë³¸ ì—°êµ¬ê°€ ìµœì´ˆì…ë‹ˆë‹¤.

- **We propose an automatic and effective end-to-end segmentation framework by a multi-scale deep network, which is robust to various scales by constructing an image pyramid to extract multi-scale features automatically.**  
  ì´ë¯¸ì§€ í”¼ë¼ë¯¸ë“œë¥¼ êµ¬ì„±í•˜ì—¬ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì— ê°•ê±´í•œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë”¥ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ìë™ì ì´ê³  íš¨ê³¼ì ì¸ ì—”ë“œ-íˆ¬-ì—”ë“œ ë¶„í•  í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

- **To obtain more accurate segmentation performance, we propose an attention-based strategy which includes channel attention and edge attention for effective feature extraction.**  
  ë³´ë‹¤ ì •í™•í•œ ë¶„í•  ì„±ëŠ¥ì„ ìœ„í•´, ì±„ë„ ì–´í…ì…˜ì™€ ì—ì§€ ì–´í…ì…˜ë¥¼ í¬í•¨í•˜ëŠ” ì–´í…ì…˜ ê¸°ë°˜ ì „ëµì„ ì œì•ˆí•˜ì—¬ íš¨ê³¼ì ì¸ íŠ¹ì§• ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- **A combined loss strategy based on multi-scale side-outputs is utilized to conduct effective deep supervision for the multi-scale network.**  
  ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‚¬ì´ë“œ ì¶œë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²°í•© ì†ì‹¤ ì „ëµì„ í†µí•´ ë„¤íŠ¸ì›Œí¬ì— íš¨ê³¼ì ì¸ í•™ìŠµ ì§€ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

- **We study different CNN-based segmentation methods for chicken segmentation on the constructed chicken database.**  
  êµ¬ì¶•ëœ ë‹­ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë‹¤ì–‘í•œ CNN ê¸°ë°˜ ë¶„í•  ë°©ë²•ì„ ì‹¤í—˜ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.

- **Experimental results demonstrate that the proposed approach achieves promising segmentation performance for chicken segmentation.**  
  ì‹¤í—˜ ê²°ê³¼, ì œì•ˆëœ ë°©ë²•ì´ ë‹­ ì´ë¯¸ì§€ ë¶„í• ì—ì„œ ìœ ë§í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

**The rest of this paper is organized as follows.**  
ë…¼ë¬¸ì˜ ë‚˜ë¨¸ì§€ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

**Related previous work is reviewed in Sec. II.**  
IIì¥ì—ì„œ ê´€ë ¨ ì„ í–‰ ì—°êµ¬ë¥¼ ê²€í† í•˜ê³ ,

**The detailed methodology is described in Sec. III.**  
IIIì¥ì—ì„œ ë°©ë²•ë¡ ì„ ìƒì„¸íˆ ì„¤ëª…í•˜ë©°,

**Experimental results are presented in Sec. IV.**  
IVì¥ì—ì„œ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì œì‹œí•˜ê³ ,

**Finally, concluding remarks and future extensions are given in Sec. V.**  
ë§ˆì§€ë§‰ìœ¼ë¡œ Vì¥ì—ì„œ ê²°ë¡ ê³¼ í–¥í›„ ì—°êµ¬ ë°©í–¥ì„ ë…¼ì˜í•©ë‹ˆë‹¤.

---

![image](https://github.com/user-attachments/assets/e6525448-af37-42db-a150-1df97906e85e)


---
---

# II. RELATED WORK  ê´€ë ¨ ì—°êµ¬

**The behavioural research of animal can be greatly simplified by automatic and intelligent monitoring systems.**  
ë™ë¬¼ì˜ í–‰ë™ ì—°êµ¬ëŠ” ìë™í™”ë˜ê³  ì§€ëŠ¥ì ì¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ í†µí•´ í¬ê²Œ ê°„ì†Œí™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Recently, methods based on computer vision [5], [9] have shown the advantages to conduct effective evaluation without influence to the normal behaviour of the animals.**  
ìµœê·¼ ì»´í“¨í„° ë¹„ì „ ê¸°ë°˜ ë°©ë²• [5], [9]ì€ ë™ë¬¼ì˜ ì •ìƒ í–‰ë™ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ íš¨ê³¼ì ì¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì¥ì ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.

**Animal segmentation is the basic and important step to monitor and analyze each animal separately.**  
ë™ë¬¼ ë¶„í• ì€ ê°œë³„ ë™ë¬¼ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¶„ì„í•˜ê¸° ìœ„í•œ ê¸°ì´ˆì ì´ê³  ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.

---

**A. ANIMAL DETECTION AND ANALYSE**  
**A. ë™ë¬¼ íƒì§€ ë° ë¶„ì„**

**Many object detection and segmentation methods have been proposed to detect the individual animal [3], [4], [7], [10]â€“[12].**  
ê°œë³„ ë™ë¬¼ì„ íƒì§€í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê°ì²´ íƒì§€ ë° ë¶„í•  ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤ [3], [4], [7], [10]â€“[12].

**Kashiha et al. [10] proposed to label image and distinguish pigs by Otsu segmentation method.**  
Kashiha ë“± [10]ì€ Otsu ë¶„í•  ë°©ë²•ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ì— ë¼ë²¨ì„ ë¶™ì´ê³  ë¼ì§€ë¥¼ êµ¬ë³„í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Guo et al. [11] proposed a pig segmentation method by combining the mixed Gaussian model with the maximum entropy.**  
Guo ë“± [11]ì€ í˜¼í•© ê°€ìš°ì‹œì•ˆ ëª¨ë¸ê³¼ ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ë¥¼ ê²°í•©í•œ ë¼ì§€ ë¶„í•  ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Guo et al. [12] proposed a multi-level threshold segmentation approach to obtain pigâ€™s profile.**  
Guo ë“± [12]ì€ ë‹¤ë‹¨ê³„ ì„ê³„ê°’ ë¶„í•  ê¸°ë²•ì„ í†µí•´ ë¼ì§€ì˜ ìœ¤ê³½ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Vroegindeweij et al. [7] proposed a method for pixel segmentation based on spectral reflectance properties.**  
Vroegindeweij ë“± [7]ì€ ìŠ¤í™íŠ¸ëŸ¼ ë°˜ì‚¬ íŠ¹ì„±ì— ê¸°ë°˜í•œ í”½ì…€ ë‹¨ìœ„ ë¶„í•  ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Nilsson et al. [4] proposed to segment pigs from image by a structured prediction approach with several extracted image features.**  
Nilsson ë“± [4]ì€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ì¶”ì¶œí•œ ë’¤, êµ¬ì¡°ì  ì˜ˆì¸¡ ì ‘ê·¼ë²•ìœ¼ë¡œ ë¼ì§€ë¥¼ ë¶„í• í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Nasirahmadi et al. [3] described depth sensor based imaging systems along with 2D cameras for effectively identifying livestock behaviours, and presented automatic approach for monitoring cattles and pigs.**  
Nasirahmadi ë“± [3]ì€ 2D ì¹´ë©”ë¼ì™€ ê¹Šì´ ì„¼ì„œë¥¼ í™œìš©í•œ ì˜ìƒ ì‹œìŠ¤í…œì„ í†µí•´ ê°€ì¶•ì˜ í–‰ë™ì„ íš¨ê³¼ì ìœ¼ë¡œ ì‹ë³„í•˜ê³ , ì†Œì™€ ë¼ì§€ë¥¼ ìë™ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Most segmentation methods mentioned above segment pigs or other farmed animals with sensor-based or traditional machine learning techniques.**  
ìœ„ì— ì–¸ê¸‰ëœ ëŒ€ë¶€ë¶„ì˜ ë¶„í•  ë°©ë²•ì€ ì„¼ì„œ ê¸°ë°˜ ë˜ëŠ” ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ë¼ì§€ë‚˜ ê¸°íƒ€ ê°€ì¶•ì„ ë¶„í• í•©ë‹ˆë‹¤.

<br>

**Recenlty, CNN based methods [5], [6], [8], [9], [13], [14] have been proposed to study the behavior of animals.**  
ìµœê·¼ì—ëŠ” ë™ë¬¼ì˜ í–‰ë™ì„ ì—°êµ¬í•˜ê¸° ìœ„í•´ CNN ê¸°ë°˜ì˜ ë°©ë²• [5], [6], [8], [9], [13], [14]ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

**Cowton et al. [5] combined Faster R-CNN with two potential real-time multi-object tracking methods to autonomously localise and track individual pigs from RGB cameras.**  
Cowton ë“± [5]ì€ Faster R-CNNê³¼ ë‘ ê°€ì§€ ì‹¤ì‹œê°„ ë‹¤ì¤‘ ê°ì²´ ì¶”ì  ê¸°ë²•ì„ ê²°í•©í•˜ì—¬ RGB ì¹´ë©”ë¼ë¡œ ê°œë³„ ë¼ì§€ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì í–ˆìŠµë‹ˆë‹¤.

**Tian et al. [9] proposed a deep learning solution to address the pig counting problem by modifying counting convolutional neural network model according to the structure of ResNeXt [15].**  
Tian ë“± [9]ì€ ResNeXt êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹´ìš´íŒ… CNN ëª¨ë¸ì„ ìˆ˜ì •í•˜ì—¬ ë¼ì§€ ìˆ˜ë¥¼ ì„¸ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë”¥ëŸ¬ë‹ ì†”ë£¨ì…˜ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**The proposed CNN model learns the mapping from the image features to the density map, and obtains the total number of pigs in the entire image by integrating the density map.**  
ì œì•ˆëœ CNN ëª¨ë¸ì€ ì´ë¯¸ì§€ íŠ¹ì§•ì„ ë°€ë„ ë§µìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” í•™ìŠµì„ ìˆ˜í–‰í•˜ë©°, ì´ë¥¼ í†µí•©í•˜ì—¬ ì „ì²´ ì´ë¯¸ì§€ì˜ ë¼ì§€ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

**Zhang et al. [8] proposed a CNN-based detector and a correlation filter-based tracker via a hierarchical data association algorithm to multiple pigs detection and tracking.**  
Zhang ë“± [8]ì€ ê³„ì¸µì  ë°ì´í„° ì—°ê´€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ë‹¤ìˆ˜ì˜ ë¼ì§€ë¥¼ íƒì§€í•˜ê³  ì¶”ì í•˜ê¸° ìœ„í•œ CNN ê¸°ë°˜ íƒì§€ê¸°ì™€ ìƒê´€ í•„í„° ê¸°ë°˜ ì¶”ì ê¸°ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Yang et al. [13] used Faster R-CNN to locate and identify individual pig.**  
Yang ë“± [13]ì€ Faster R-CNNì„ í™œìš©í•˜ì—¬ ê°œë³„ ë¼ì§€ë¥¼ íƒì§€í•˜ê³  ì‹ë³„í–ˆìŠµë‹ˆë‹¤.

**Zheng et al. [6] introduced Faster R-CNN framework to identify five postures and obtain sows accurate location in loose pens.**  
Zheng ë“± [6]ì€ Faster R-CNNì„ ì‚¬ìš©í•˜ì—¬ í—›ê°„ ì•ˆì—ì„œì˜ ì•”í‡˜ì§€ ìì„¸ ë‹¤ì„¯ ê°€ì§€ë¥¼ ì‹ë³„í•˜ê³  ì •í™•í•œ ìœ„ì¹˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Hansen et al. [14] used Fisherface and CNN model for pig face recognition.**  
Hansen ë“± [14]ì€ Fisherfaceì™€ CNN ëª¨ë¸ì„ ì´ìš©í•œ ë¼ì§€ ì–¼êµ´ ì¸ì‹ ë°©ë²•ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

---

**Generally, current deep learning based segmentation methods can be roughly categorized into two classes.**  
ì¼ë°˜ì ìœ¼ë¡œ, í˜„ì¬ì˜ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ë¶„í•  ë°©ë²•ì€ ë‘ ê°€ì§€ë¡œ êµ¬ë¶„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**One is detection based segmentation methods [16]â€“[18] and the other is direct segmentation methods [19]â€“[25].**  
í•˜ë‚˜ëŠ” íƒì§€ ê¸°ë°˜ ë¶„í•  ë°©ë²• [16]â€“[18], ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì§ì ‘ ë¶„í•  ë°©ë²• [19]â€“[25]ì…ë‹ˆë‹¤.

**Detection based methods first obtain the region of each instance, and then predict the mask for each region.**  
íƒì§€ ê¸°ë°˜ ë°©ë²•ì€ ë¨¼ì € ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ì˜ì—­ì„ ì°¾ê³ , ì´í›„ í•´ë‹¹ ì˜ì—­ì— ëŒ€í•´ ë§ˆìŠ¤í¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

**Direct segmentation methods predict the label of each pixel to form instance segmentation results directly.**  
ì§ì ‘ ë¶„í•  ë°©ë²•ì€ ê° í”½ì…€ì˜ ë ˆì´ë¸”ì„ ë°”ë¡œ ì˜ˆì¸¡í•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ë¶„í•  ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

---

**B. DETECTION BASED SEGMENTATION METHODS**  
**B. íƒì§€ ê¸°ë°˜ ë¶„í•  ë°©ë²•**

**The Region-based Convolutional Neural Network method (RCNN) [26] achieved excellent object detection accuracy by using a deep ConvNet to classify object proposals.**  
RCNN [26]ì€ ê¹Šì€ ConvNetì„ í™œìš©í•´ ê°ì²´ í›„ë³´ë¥¼ ë¶„ë¥˜í•¨ìœ¼ë¡œì¨ ë›°ì–´ë‚œ ê°ì²´ íƒì§€ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

**Then, Ross Girshick [27] proposed a Fast-RCNN for object detection.**  
ì´í›„ Ross Girshick [27]ì€ ê°ì²´ íƒì§€ë¥¼ ìœ„í•œ Fast-RCNNì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Based on Fast-RCNN, Ren et al. [28] proposed Faster-RCNN method by combining a region proposal network with Fast-RCNN into a single network by sharing their convolutional features.**  
Fast-RCNNì„ ê¸°ë°˜ìœ¼ë¡œ, Ren ë“± [28]ì€ ì˜ì—­ ì œì•ˆ ë„¤íŠ¸ì›Œí¬ë¥¼ Fast-RCNNê³¼ í†µí•©í•˜ì—¬ í•©ì„±ê³± íŠ¹ì§•ì„ ê³µìœ í•˜ëŠ” Faster-RCNNì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**These methods just focus on object detection.**  
ì´ë“¤ ë°©ë²•ì€ ê°ì²´ íƒì§€ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.

**Mask-RCNN [16], as an effective detect-then-segment methodology, is a very successful instantiation.**  
Mask-RCNN [16]ì€ íƒì§€ í›„ ë¶„í•  ë°©ì‹ì˜ ì„±ê³µì ì¸ ì‚¬ë¡€ì…ë‹ˆë‹¤.

**Mask-RCNN extends Faster-RCNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI).**  
Mask-RCNNì€ Faster-RCNNì— ê° ê´€ì‹¬ ì˜ì—­(RoI)ì— ëŒ€í•œ ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¶„ê¸°ë¥¼ ì¶”ê°€í•˜ì—¬ í™•ì¥í•œ ê²ƒì…ë‹ˆë‹¤.

**Chen et al. [17] proposed MaskLab by using position sensitive scores for object segmentation.**  
Chen ë“± [17]ì€ ìœ„ì¹˜ ë¯¼ê° ì ìˆ˜ë¥¼ í™œìš©í•œ ê°ì²´ ë¶„í•  ë°©ë²•ì¸ MaskLabì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Huang et al. [18] proposed Mask Scoring RCNN to learn the quality of the predicted instance masks with a network block.**  
Huang ë“± [18]ì€ ì˜ˆì¸¡ëœ ì¸ìŠ¤í„´ìŠ¤ ë§ˆìŠ¤í¬ì˜ í’ˆì§ˆì„ í•™ìŠµí•˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¸”ë¡ì¸ Mask Scoring RCNNì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Chen et al. [29] proposed TensorMask framework by establishing the first dense sliding window instance segmentation system.**  
Chen ë“± [29]ì€ ìµœì´ˆì˜ ì¡°ë°€í•œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜ ì¸ìŠ¤í„´ìŠ¤ ë¶„í•  ì‹œìŠ¤í…œì¸ TensorMask í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**However, an underlying drawback in these methods is that mask quality depends on the classification scores.**  
í•˜ì§€ë§Œ ì´ë“¤ ë°©ë²•ì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œëŠ” ë§ˆìŠ¤í¬ í’ˆì§ˆì´ ë¶„ë¥˜ ì ìˆ˜ì— ì˜ì¡´í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

**It is inappropriate to use classification confidence to evaluate the mask quality.**  
ë¶„ë¥˜ ì‹ ë¢°ë„ë¥¼ ë§ˆìŠ¤í¬ í’ˆì§ˆ í‰ê°€ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì ì ˆí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

---

**C. DIRECT SEGMENTATION METHODS**  
**C. ì§ì ‘ ë¶„í•  ë°©ë²•**

**Recently, the direct segmentation methods without depending on the detected bounding box have drawn lots of attentions, such as FCNs [30], SegNet [31], and Unet [19].**  
ìµœê·¼ì—ëŠ” íƒì§€ëœ ë°”ìš´ë”© ë°•ìŠ¤ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ì§ì ‘ ë¶„í• ì„ ìˆ˜í–‰í•˜ëŠ” FCN [30], SegNet [31], Unet [19] ë“±ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.

**These approaches can produce pixel-wise dense prediction by an encoder-decoder architecture.**  
ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ í†µí•´ í”½ì…€ ë‹¨ìœ„ì˜ ì¡°ë°€í•œ ì˜ˆì¸¡ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Generally, an encoder-decoder architecture consists of a contracting (encoder) and expanding (decoder) sub-network.**  
ì¼ë°˜ì ìœ¼ë¡œ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ëŠ” ì¶•ì†Œ ë„¤íŠ¸ì›Œí¬(ì¸ì½”ë”)ì™€ í™•ì¥ ë„¤íŠ¸ì›Œí¬(ë””ì½”ë”)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

**The encoder captures global context, while the decoder fuses global info with local details to generate segmentation.**  
ì¸ì½”ë”ëŠ” ì „ì²´ ì´ë¯¸ì§€ì˜ ì „ì—­ ë§¥ë½ì„ í¬ì°©í•˜ê³ , ë””ì½”ë”ëŠ” ì „ì—­ ì •ë³´ì™€ ì§€ì—­ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë¶„í• ì„ ìƒì„±í•©ë‹ˆë‹¤.

**Unet can handle data scarcity and class imbalance, and can be extended to other computer vision tasks.**  
Unetì€ ë°ì´í„° ë¶€ì¡±, í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©° ë‹¤ë¥¸ ë¹„ì „ ì‘ì—…ì—ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**However, Unet only uses the original image and ignores context guidance and multi-scale info.**  
í•˜ì§€ë§Œ Unetì€ ì›ë³¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©í•˜ë©°, ë§¥ë½ ì •ë³´ë‚˜ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì •ë³´ë¥¼ í™œìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

<br>

**Different Unet variants [20]â€“[25] have been proposed for better segmentation.**  
ë” ë‚˜ì€ ë¶„í• ì„ ìœ„í•´ ë‹¤ì–‘í•œ Unet ë³€í˜• [20]â€“[25]ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.

**Multi-scale information has been shown to improve performance.**  
ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì •ë³´ëŠ” ë¶„í•  ì„±ëŠ¥ í–¥ìƒì— íš¨ê³¼ì ì¸ ê²ƒìœ¼ë¡œ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤.

**Yang et al. [23] proposed CLCINet, a cross-level fusion and context inference network.**  
Yang ë“± [23]ì€ ë ˆë²¨ ê°„ ìœµí•©ê³¼ ë¬¸ë§¥ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” CLCINetì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Zhou et al. [24] proposed D-UNet, fusing 2D and 3D convolutions in encoding.**  
Zhou ë“± [24]ì€ ì¸ì½”ë”© ë‹¨ê³„ì—ì„œ 2Dì™€ 3D í•©ì„±ê³±ì„ ìœµí•©í•œ D-UNetì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

**Qi et al. [25] proposed X-Net using depthwise separable convolution and a feature similarity module.**  
Qi ë“± [25]ì€ ê¹Šì´ ë¶„ë¦¬ í•©ì„±ê³±ê³¼ íŠ¹ì§• ìœ ì‚¬ì„± ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” X-Netì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

---

**There is a large demand for intelligent farming of chickens and pigs.**  
ë‹­ê³¼ ë¼ì§€ë¥¼ ìœ„í•œ ì§€ëŠ¥í˜• ë†ì—…ì— ëŒ€í•œ ìˆ˜ìš”ê°€ í½ë‹ˆë‹¤.

**Though some pig segmentation methods exist, chicken segmentation is less explored.**  
ì¼ë¶€ ë¼ì§€ ë¶„í•  ê¸°ë²•ì´ ì¡´ì¬í•˜ì§€ë§Œ ë‹­ì— ëŒ€í•œ ìë™ ë¶„í• ê³¼ ë¶„ì„ì€ ê±°ì˜ ì—°êµ¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.

**Chicken segmentation is more challenging due to its small size and flocking behavior.**  
ë‹­ì€ í¬ê¸°ê°€ ì‘ê³  êµ°ì§‘ í–‰ë™ì„ í•˜ê¸°ì— ë¶„í• ì´ ë” ì–´ë µìŠµë‹ˆë‹¤.

**In this work, we propose an automatic deep network framework for effective chicken image segmentation.**  
ì´ì— ë³¸ ì—°êµ¬ì—ì„œëŠ” íš¨ê³¼ì ì¸ ë‹­ ì´ë¯¸ì§€ ë¶„í• ì„ ìœ„í•œ ìë™ ë”¥ ë„¤íŠ¸ì›Œí¬ í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

---

![image](https://github.com/user-attachments/assets/44297658-d998-4d0d-8e70-b981cf07c7c0)

> - **Otsu ë¶„í• ** : OtsuëŠ” ì´ë¯¸ì§€ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•´, **ê°€ì¥ ì˜ êµ¬ë¶„ë˜ëŠ”(í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì´ ê°€ì¥ í°) ì„ê³„ê°’**ì„ ìë™ìœ¼ë¡œ ê³„ì‚°í•´ì£¼ëŠ” ì´ì§„í™” ë°©ë²•

> - **ê°€ìš°ì‹œì•ˆ í˜¼í•©(GMM)** : ë°ì´í„°ê°€ ì—¬ëŸ¬ ê°œì˜ **ì •ê·œë¶„í¬(ê°€ìš°ì‹œì•ˆ ë¶„í¬)** ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , ê° ë¶„í¬ë¥¼ ì°¾ì•„ë‚´ì–´ ì „ì²´ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•

> - **ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì„ê³„ê°’** : í”½ì…€ ë¶„í¬ì˜ ì •ë³´ëŸ‰(ì—”íŠ¸ë¡œí”¼)ì„ ê¸°ì¤€ìœ¼ë¡œ, **ì „ì²´ ì—”íŠ¸ë¡œí”¼ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ì§€ì **ì„ ì„ê³„ê°’ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ë°©ë²•. ë°°ê²½ê³¼ ê°ì²´ê°€ ì„œë¡œ ê°€ì¥ êµ¬ë³„ë˜ë„ë¡ ì •ë³´ë¥¼ ë¶„ë¦¬í•˜ë ¤ëŠ” ëª©ì ì„ ê°€ì§

> - **ìŠ¤í™íŠ¸ëŸ¼ ë°˜ì‚¬ ë¶„ì„** : ë¬¼ì²´ì˜ **ì¬ì§ˆ ê³ ìœ ì˜ ë¹› ë°˜ì‚¬ íŠ¹ì„±(ìŠ¤í™íŠ¸ëŸ¼ ë°˜ì‚¬ìœ¨)** ì„ ë¶„ì„í•˜ì—¬ ë¬¼ì²´ë¥¼ êµ¬ë¶„í•˜ëŠ” ê¸°ë²•. íŠ¹ì • íŒŒì¥ëŒ€ì˜ ë°˜ì‚¬ íŠ¹ì„±ì„ ì´ìš©í•´ ë™ì¼í•œ ìƒ‰ìƒì´ë¼ë„ ì¬ì§ˆì´ ë‹¤ë¥´ë©´ êµ¬ë¶„ ê°€ëŠ¥í•¨ (ì˜ˆ: ì ì™¸ì„ /ê·¼ì ì™¸ì„  ì˜ìƒ ë“±ì—ì„œ ì‚¬ìš©)

---

![image](https://github.com/user-attachments/assets/fc6b31af-14a6-47bf-89ee-8770e5ffdad3)

> - **R-CNN (Region-based CNN)** :  ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ **Selective Search**ë¡œ ìˆ˜ì²œ ê°œì˜ í›„ë³´ ì˜ì—­(region proposal)ì„ ìƒì„±í•˜ê³ , ê° ì˜ì—­ì„ ì˜ë¼ì„œ CNNì— í†µê³¼ì‹œì¼œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  SVMìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹.  
>   â†’ CNN ê¸°ë°˜ì˜ ì²« ê°ì²´ íƒì§€ ëª¨ë¸  
>   â†’ ë‹¨ì  : ì†ë„ê°€ ë§¤ìš° ëŠë¦¼ (ê° ì˜ì—­ì„ **í•˜ë‚˜ì”© CNNì— ì…ë ¥**)

> - **Fast R-CNN** : ì „ì²´ ì´ë¯¸ì§€ë¥¼ CNNì— **í•œ ë²ˆë§Œ í†µê³¼ì‹œì¼œ feature map ìƒì„±**, ì´í›„ í›„ë³´ ì˜ì—­(ROI)ì„ feature mapì—ì„œ ì˜ë¼ë‚´ì–´ ë¶„ë¥˜í•˜ê³  ìœ„ì¹˜ë¥¼ ë³´ì •í•¨.  
>   â†’ **R-CNNë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„**  
>   â†’ **R-CNN ëŒ€ë¹„ ë°œì „í•œ ì ** : ì´ë¯¸ì§€ ì „ì²´ì— ëŒ€í•´ CNN **1íšŒë§Œ ìˆ˜í–‰**, ROI Pooling ë„ì…

> - **Faster R-CNN** : í›„ë³´ ì˜ì—­ì„ ì™¸ë¶€ ì•Œê³ ë¦¬ì¦˜ ì—†ì´, CNN ì•ˆì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” **Region Proposal Network (RPN)**ì„ ë„ì…  
>   â†’ **Fast R-CNNì˜ ë³‘ëª©ì´ì—ˆë˜ Selective Searchë¥¼ ì œê±°**  
>   â†’ **Fast R-CNN ëŒ€ë¹„ ë°œì „í•œ ì ** : ì œì•ˆ ì˜ì—­ ìƒì„±ê¹Œì§€ **ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ì—ì„œ í†µí•©**, ì „ì²´ê°€ **ì—”ë“œíˆ¬ì—”ë“œ(end-to-end) í•™ìŠµ ê°€ëŠ¥**

> - **Mask R-CNN** : Faster R-CNNì— **í”½ì…€ ë‹¨ìœ„ ë§ˆìŠ¤í¬ ë¶„í• (segmentation mask)** ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬, ê°ì²´ì˜ ìœ„ì¹˜ë¿ ì•„ë‹ˆë¼ **ì •í™•í•œ ìœ¤ê³½ê¹Œì§€ ì˜ˆì¸¡**  
>   â†’ **ê°ì²´ ì¸ìŠ¤í„´ìŠ¤ ë¶„í• (instance segmentation)** ê°€ëŠ¥  
>   â†’ **Faster R-CNN ëŒ€ë¹„ ë°œì „í•œ ì ** : ROI Pooling ëŒ€ì‹  ë” ì •í™•í•œ **ROI Align** ì‚¬ìš©

---

> - **FCN (Fully Convolutional Network)** : ì „í†µì ì¸ CNNì—ì„œ ì™„ì „ ì—°ê²°ì¸µ(FC layer)ì„ ì œê±°í•˜ê³ , **ëª¨ë“  ê³„ì¸µì„ ì»¨ë³¼ë£¨ì…˜ê³¼ ì—…ìƒ˜í”Œë§ìœ¼ë¡œë§Œ êµ¬ì„±**í•œ ì²« ë²ˆì§¸ ì‹œë§¨í‹± ë¶„í•  ë„¤íŠ¸ì›Œí¬.  
>   â†’ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê³µê°„ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©° **í”½ì…€ ë‹¨ìœ„ ì˜ˆì¸¡** ê°€ëŠ¥  
>   â†’ **ê¸°ì´ˆ ëª¨ë¸**ë¡œ, ì´í›„ ë§ì€ ë¶„í•  ë„¤íŠ¸ì›Œí¬ì˜ ê¸°ë°˜ì´ ë¨

> - **SegNet** : FCN êµ¬ì¡°ì—ì„œ ì—…ìƒ˜í”Œë§ì„ ê°œì„ í•œ êµ¬ì¡°ë¡œ, **ë””ì½”ë”ì—ì„œ í’€ë§ ì¸ë±ìŠ¤ë¥¼ ì¬ì‚¬ìš©**í•˜ì—¬ ë” ì •í™•í•˜ê²Œ ë³µì›í•¨.  
>   â†’ í’€ë§ ì‹œ ì €ì¥í•œ ì¸ë±ìŠ¤ë¥¼ ë””ì½”ë” ì—…ìƒ˜í”Œë§ì— ê·¸ëŒ€ë¡œ ì‚¬ìš©  
>   â†’ **FCN ëŒ€ë¹„ ë°œì „í•œ ì ** : **ì—…ìƒ˜í”Œë§ ì •í™•ë„ í–¥ìƒ**, ê³„ì‚°ëŸ‰ ì ˆê°

> - **U-Net** : FCN êµ¬ì¡°ì— **ëŒ€ì¹­ì ì¸ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**ì™€ **ìŠ¤í‚µ ì—°ê²°(skip connection)**ì„ ë„ì…í•˜ì—¬ ì €í•´ìƒë„ì—ì„œ ì†ì‹¤ëœ ì •ë³´ë¥¼ ë³µì›  
>   â†’ ìƒë¬¼ ì˜í•™ ì˜ìƒ ë¶„í• ì—ì„œ ë§¤ìš° ë›°ì–´ë‚œ ì„±ëŠ¥  
>   â†’ **SegNet ëŒ€ë¹„ ë°œì „í•œ ì ** : **ìŠ¤í‚µ ì—°ê²° ë„ì…**, **ê²½ê³„ ì •ë³´ ë³´ì¡´ ê°•í™”**, **ì†ŒëŸ‰ ë°ì´í„° ëŒ€ì‘ë ¥ ìš°ìˆ˜**

> - **CLCI-Net (Cross-Level Context Integration Network)** : ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜ íŠ¹ì§•ì„ í†µí•©í•˜ì—¬ ì •ë°€í•œ ê²½ê³„ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„¤íŠ¸ì›Œí¬  
>   â†’ **ì €ìˆ˜ì¤€ ì •ë³´(ìì„¸í•œ ê²½ê³„)** ì™€ **ê³ ìˆ˜ì¤€ ì •ë³´(ì˜ë¯¸ì  ì˜ë¯¸)** ë¥¼ ê²°í•©í•˜ì—¬ **ì •í™•ë„ í–¥ìƒ**  
>   â†’ **U-Net ëŒ€ë¹„ ë°œì „í•œ ì ** : **ë‹¤ì¤‘ ë ˆë²¨ íŠ¹ì§• ìœµí•©(Cross-Level Context)**, **ê²½ê³„ ì„¸ë¶„í™” ì •ë°€ë„ í–¥ìƒ**

> - **D-UNet (Dilated U-Net)** : U-Netì— **íŒ½ì°½í•©ì„±ê³±(Dilated Convolution)** ì„ ì ìš©í•˜ì—¬, **ìˆ˜ìš©ì˜ì—­ì„ í™•ì¥**í•˜ë©´ì„œë„ í•´ìƒë„ ì†ì‹¤ ì—†ì´ íŠ¹ì§•ì„ ì¶”ì¶œ  
>   â†’ ë” ë„“ì€ ë¬¸ë§¥ ì •ë³´ë¥¼ í™œìš©í•´ ë¶„í•  ì„±ëŠ¥ í–¥ìƒ  
>   â†’ **U-Net ëŒ€ë¹„ ë°œì „í•œ ì ** : **íŒ½ì°½í•©ì„±ê³±ì„ í†µí•œ ë¬¸ë§¥ ê°•í™”**, **ë„“ì€ ë²”ìœ„ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ìš©**

> - **X-Net** : U-Net êµ¬ì¡°ì—ì„œ **ì¤‘ê°„ ë””ì½”ë”ë¥¼ ì¶”ê°€ë¡œ ì‚½ì…**í•˜ì—¬ ë‹¤ì¤‘ ë””ì½”ë”© ê²½ë¡œë¥¼ ê°–ëŠ” í™•ì¥ êµ¬ì¡°  
>   â†’ ì—¬ëŸ¬ í•´ìƒë„ì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ì—¬ **ë” ì •ë°€í•œ ë¶„í• ** ê°€ëŠ¥  
>   â†’ **U-Net ëŒ€ë¹„ ë°œì „í•œ ì ** : **ë‹¤ì¤‘ ë””ì½”ë” êµ¬ì¡°**, **í”¼ë“œë°± ì •ë³´ í™œìš© ê°•í™”**

---
---


## **III. METHODOLOGY ë°©ë²•ë¡ **

**In this part, we introduce the proposed multi-scale attention based MSAnet method for chicken segmentation.**  
ì´ ì¥ì—ì„œëŠ” ë‹­ ì´ë¯¸ì§€ ë¶„í• ì„ ìœ„í•œ ì œì•ˆëœ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì–´í…ì…˜ ê¸°ë°˜ **MSAnet** ë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.

**Fig. 3 illustrates the flowchart of our MSAnet segmentation method.**  
Fig. 3ì€ ì œì•ˆëœ MSAnet ë¶„í•  ë°©ë²•ì˜ ì „ì²´ íë¦„ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
<p align="center">
    <img src="https://github.com/user-attachments/assets/5fe9cf46-b5ac-4c61-85fd-bb090ef2aff7" alt="image" width="500">
</p>

**The proposed method is an end-to-end deep network with four main parts.**  
ì œì•ˆëœ ë°©ë²•ì€ ë„¤ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì§„ ì—”ë“œ-íˆ¬-ì—”ë“œ ë”¥ ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.

**First, the MSAnet framework is an encoder-decoder structure network, which aims to learn effective hierarchical representation.**  
ì²«ì§¸, MSAnetì€ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ì˜ ë„¤íŠ¸ì›Œí¬ë¡œ, íš¨ê³¼ì ì¸ ê³„ì¸µì  í‘œí˜„ í•™ìŠµì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

**Second, we use a multi-scale module to construct an image pyramid input and achieve multi-level receptive field fusion for effective feature extraction.**  
ë‘˜ì§¸, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í”¼ë¼ë¯¸ë“œë¥¼ ì…ë ¥ìœ¼ë¡œ êµ¬ì„±í•˜ê³ , ë‹¤ë‹¨ê³„ ìˆ˜ìš© ì˜ì—­ ìœµí•©ì„ í†µí•´ íš¨ê³¼ì ì¸ íŠ¹ì§• ì¶”ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**Then the double attention module composed of channel attention and edge attention, can obtain global and local information for segmentation.**  
ì…‹ì§¸, ì±„ë„ ì–´í…ì…˜ì™€ ì—ì§€ ì–´í…ì…˜ë¡œ êµ¬ì„±ëœ ì´ì¤‘ ì–´í…ì…˜ ëª¨ë“ˆì€ ë¶„í• ì„ ìœ„í•œ ì „ì—­ ë° ì§€ì—­ ì •ë³´ë¥¼ ë™ì‹œì— íšë“í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Last, the combined loss based on multi-scale side-outputs is utilized to provide effective supervision.**  
ë§ˆì§€ë§‰ìœ¼ë¡œ, ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‚¬ì´ë“œ ì¶œë ¥ì— ê¸°ë°˜í•œ ê²°í•© ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ê³¼ì ì¸ í•™ìŠµ ì§€ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**The details of our MSAnet are illustrated as follows.**  
ë‹¤ìŒì—ì„œ MSAnetì˜ ì„¸ë¶€ ë‚´ìš©ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

### **A. ENCODER-DECODER STRUCTURE NETWORK**  
**A. ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡° ë„¤íŠ¸ì›Œí¬**

**As shown in Fig. 3, the proposed MSAnet framework is an end-to-end U-shape network composed by an encoder part and a decoder part.**  
**Fig. 3**ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì œì•ˆëœ MSAnet í”„ë ˆì„ì›Œí¬ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë”ë¡œ êµ¬ì„±ëœ ì—”ë“œ-íˆ¬-ì—”ë“œ Uìí˜• ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.

**The encoder part performs convolution layer with a filter bank to produce a set of encoder feature maps,**  
ì¸ì½”ë” ë¶€ë¶„ì€ í•„í„° ë±…í¬ì™€ í•©ì„±ê³± ê³„ì¸µì„ í†µí•´ ì¸ì½”ë” íŠ¹ì§• ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.

**while the decoder part utilizes up-sampling and feature enhancement operations to output feature maps for effective image segmentation.**  
ë°˜ë©´ ë””ì½”ë” ë¶€ë¶„ì€ ì—…ìƒ˜í”Œë§ê³¼ íŠ¹ì§• ê°•í™” ì—°ì‚°ì„ ì´ìš©í•˜ì—¬ íš¨ê³¼ì ì¸ ì´ë¯¸ì§€ ë¶„í• ì„ ìœ„í•œ ì¶œë ¥ íŠ¹ì§• ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.

---

### **B. MULTI-SCALE MODULE**  
**B. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ëª¨ë“ˆ**


**To deal with the various scales of chickens, we utilize multi-scale module for chicken segmentation by constructing a multi-scale input in the encoder path.**  
ë‹­ì˜ ë‹¤ì–‘í•œ í¬ê¸°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´, ì¸ì½”ë” ê²½ë¡œì—ì„œ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì…ë ¥ì„ êµ¬ì„±í•˜ì—¬ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ëª¨ë“ˆì„ ì ìš©í•©ë‹ˆë‹¤.

**The multi-scale module has been proved effectively for image segmentation in work [21].**  
ì´ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ëª¨ë“ˆì€ ì´ë¯¸ì§€ ë¶„í• ì—ì„œ íš¨ê³¼ì ì¸ ê²ƒìœ¼ë¡œ [21]ì—ì„œ ì…ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.

**As shown in Fig. 3, two kinds of features can be integrated both from the vertical direction and horizontal direction for the ith layer of MSAnet, where i âˆˆ [2n].**  
ê·¸ë¦¼ 3ì—ì„œì™€ ê°™ì´ MSAnetì˜ ië²ˆì§¸ ì¸µì—ì„œëŠ” ìˆ˜ì§ ë°©í–¥ê³¼ ìˆ˜í‰ ë°©í–¥ì—ì„œ íŠ¹ì§•ë“¤ì„ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ iëŠ” [2n] ë²”ìœ„ ë‚´ì˜ ì •ìˆ˜ì…ë‹ˆë‹¤.

---

**The process for each layer can be described as**  
ê° ì¸µì˜ ì²˜ë¦¬ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:
<p align="center">
    <img src="https://github.com/user-attachments/assets/2e27c07c-66b3-4c0f-9896-6e3aef94b1af" alt="image" width="400">
</p>

---

**where $I$ denotes the original chicken image, and $I_i$ denotes the $i$-th down-sampled image of $I$. $n$ is the total layer number.**  
ì—¬ê¸°ì„œ $I$ëŠ” ì›ë³¸ ë‹­ ì´ë¯¸ì§€, $I_i$ëŠ” $I$ì˜ $i$ë²ˆì§¸ ë‹¤ìš´ìƒ˜í”Œë§ ì´ë¯¸ì§€ì´ë©°, $n$ì€ ì´ ì¸µ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

**The function $\mathrm{Cat}()$ denotes the concatenate operation in the channel direction.**  
$\mathrm{Cat}()$ í•¨ìˆ˜ëŠ” ì±„ë„ ë°©í–¥ìœ¼ë¡œì˜ ì—°ê²°(concatenate) ì—°ì‚°ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**$\mathrm{CR}()$ denotes one (Conv+ReLU) layer for feature extraction while $\mathrm{CR2}()$ denotes two (Conv+ReLU) layers.**  
$\mathrm{CR}()$ëŠ” íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ í•˜ë‚˜ì˜ Conv+ReLU ê³„ì¸µì„, $\mathrm{CR2}()$ëŠ” ë‘ ê°œì˜ Conv+ReLU ê³„ì¸µì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**$F_{v}^{i-1}$ denotes the features obtained from the $(i-1)$-th layer in the vertical direction as shown in Fig. 3.**  
$F_{v}^{i-1}$ëŠ” Fig. 3ì—ì„œ ë³´ì´ë“¯ ìˆ˜ì§ ë°©í–¥ìœ¼ë¡œ $(i-1)$ë²ˆì§¸ ì¸µì—ì„œ ì–»ì–´ì§„ íŠ¹ì§•ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**$F_i$ is the extracted feature maps of the $i$-th layer which will be fed into the corresponding attention process as shown in Fig. 3.**  
$F_i$ëŠ” $i$ë²ˆì§¸ ì¸µì—ì„œ ì¶”ì¶œëœ íŠ¹ì§• ë§µì´ë©°, Fig. 3ì— ë‚˜íƒ€ë‚œ í•´ë‹¹ attention ëª¨ë“ˆë¡œ ì…ë ¥ë©ë‹ˆë‹¤.

---

**The multi-scale module has the following advantages for chicken segmentation:**  
ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ëª¨ë“ˆì€ ë‹­ ì´ë¯¸ì§€ ë¶„í• ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ê°€ì§‘ë‹ˆë‹¤:

**1) dealing with various appearance generated by fast-growing process of chicken and making the scheme robust to different scales;**  
1\) ë‹­ì˜ ë¹ ë¥¸ ì„±ì¥ ê³¼ì •ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¤ì–‘í•œ ì™¸í˜•ì„ ì²˜ë¦¬í•˜ê³ , ë‹¤ì–‘í•œ í¬ê¸°ì— ê°•ê±´í•œ êµ¬ì¡°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

**2) light-weight parameters by integrating multi-scale images into the decoder layers to alleviate the large growth of parameters;**  
2) ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì´ë¯¸ì§€ë¥¼ ë””ì½”ë” ì¸µì— í†µí•©í•¨ìœ¼ë¡œì¨, ë§¤ê°œë³€ìˆ˜ ìˆ˜ì˜ ê³¼ë„í•œ ì¦ê°€ë¥¼ ì¤„ì´ëŠ” ê²½ëŸ‰í™” êµ¬ì¡°ì…ë‹ˆë‹¤.

**3) providing effective information for better feature extraction and network supervision with side-outputs**  
3) ì‚¬ì´ë“œ ì¶œë ¥(side-outputs)ì„ í†µí•´ ë” ë‚˜ì€ íŠ¹ì§• ì¶”ì¶œê³¼ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°ë…ì„ ìœ„í•œ íš¨ê³¼ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

---

## **C. ATTENTION MECHANISM  ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**

**Recently attention mechanism has demonstrated its promising performance in many tasks [34]â€“[37].**  
ìµœê·¼ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤ [34]â€“[37].

**It is challenging to segment each chicken accurately from the occluded regions due to the similar appearances of grouped chickens.**  
ë¬´ë¦¬ì§€ì–´ ìˆëŠ” ë‹­ë“¤ì˜ ìœ ì‚¬í•œ ì™¸í˜• ë•Œë¬¸ì—, ê°€ë ¤ì§„ ì˜ì—­ì—ì„œ ê°œë³„ ë‹­ì„ ì •í™•íˆ ë¶„í• í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì–´ë µìŠµë‹ˆë‹¤.

**To address this problem, a double attention strategy, which contains channel attention for global enhancement and edge attention for edge details enhancement, is proposed in this paper.**  
ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì „ì—­ íŠ¹ì§• ê°•í™”ë¥¼ ìœ„í•œ ì±„ë„ ì–´í…ì…˜(channel attention)ê³¼ ê²½ê³„ì„  ì„¸ë¶€ì •ë³´ ê°•í™”ë¥¼ ìœ„í•œ ì—£ì§€ ì–´í…ì…˜(edge attention)ìœ¼ë¡œ êµ¬ì„±ëœ ì´ì¤‘ ì–´í…ì…˜ ì „ëµì„ ì œì•ˆí•©ë‹ˆë‹¤.

**The details of the attention mechanism are introduced as below.**  
ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ì„¸ë¶€ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

---

### **Channel attention**

**In order to make the network focus on more informative features, we propose to use the channel attention to generate different attention for each channel-wise feature.**  
ë„¤íŠ¸ì›Œí¬ê°€ ë” ìœ ì˜ë¯¸í•œ íŠ¹ì§•ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡, ê° ì±„ë„ ë‹¨ìœ„ì˜ íŠ¹ì§•ë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ ì–´í…ì…˜ ê°’ì„ ìƒì„±í•˜ëŠ” ì±„ë„ ì–´í…ì…˜ ë°©ì‹ì„ ì œì•ˆí•©ë‹ˆë‹¤.

**Suppose that a feature $f$ with size of $(c, h, w)$ is the input, the channel attention module first converts $f$ to three components $X_f$, $Y_f$, and $Z_f$ via convolution operations $X()$, $Y()$ and $Z()$, respectively.**  
ì…ë ¥ íŠ¹ì§•ë§µ $f$ì˜ í¬ê¸°ê°€ $(c, h, w)$ë¼ê³  ê°€ì •í•˜ë©´, ì±„ë„ ì–´í…ì…˜ ëª¨ë“ˆì€ $f$ë¥¼ ê°ê° í•©ì„±ê³± ì—°ì‚° $X()$, $Y()$, $Z()$ì„ í†µí•´ $X_f$, $Y_f$, $Z_f$ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

$$
X_f = X(f), \quad Y_f = Y(f), \quad Z_f = Z(f)
$$

**The tensor $Y_f$ and $Z_f$ are obtained by conducting reshape operation for tensor $Y_f$ and $Z_f$ respectively. The sizes of the tensors $X_f$, $Y_f$ and $Z_f$ are $(c, h, w)$, $(c, hw)$ and $(hw, c)$, respectively.**  
$Y_f$ì™€ $Z_f$ëŠ” ê°ê° ë¦¬ì‰ì´í”„ ì—°ì‚°ì„ í†µí•´ ì–»ì–´ì§€ë©°, ì´ë“¤ì˜ í¬ê¸°ëŠ” ê°ê° $(c, h, w)$, $(c, hw)$, $(hw, c)$ì…ë‹ˆë‹¤.

**Then, attention weights $\alpha$ are computed by the matrix multiplication between $Y_f$ and $Z_f$, then fed into a softmax operation:**  
ê·¸ í›„, ì–´í…ì…˜ ê°€ì¤‘ì¹˜ $\alpha$ëŠ” $Y_f$ì™€ $Z_f$ì˜ í–‰ë ¬ê³±ì„ í†µí•´ ê³„ì‚°ë˜ê³  ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ê±°ì¹©ë‹ˆë‹¤:

$$
M = Y_f \cdot Z_f, \quad \alpha = \mathrm{softmax}(M)
$$

**$X_f$ and $\alpha$ are dot-producted to obtain the final feature map $f_{CA}$.**  
$X_f$ì™€ $\alpha$ëŠ” ë‚´ì ë˜ì–´ ìµœì¢… ì–´í…ì…˜ íŠ¹ì§• ë§µ $f_{CA}$ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

$$
f_{CA} = X_f \cdot \alpha
$$

**Fig. 4 shows the pipeline of channel attention.**  
Fig. 4ëŠ” ì±„ë„ ì–´í…ì…˜ì˜ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.  
â†’ ğŸ“Œ **[Figure 4 ì‚½ì… ìœ„ì¹˜]**

**The channel attention aims to enhance the channels with learned weight parameters in a global manner.**  
ì±„ë„ ì–´í…ì…˜ì€ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì—­ì ìœ¼ë¡œ ì±„ë„ì„ ê°•í™”í•˜ëŠ” ë° ëª©ì ì´ ìˆìŠµë‹ˆë‹¤.

**We empirically set the parameters of $X()$, $Y()$, and $Z()$ to 1-layer convolutions with 64 channels and $3\times3$ kernel sizes.**  
$X()$, $Y()$, $Z()$ëŠ” ê°ê° 64ì±„ë„ì˜ $3 \times 3$ ì»¤ë„ì„ ê°–ëŠ” 1-layer í•©ì„±ê³±ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.

---

### **Edge attention**

**Inspired by [38], [39], we propose to use edge attention to recover detailed edge information.**  
[38], [39]ì—ì„œ ì˜ê°ì„ ë°›ì•„ ì—£ì§€ ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì •ë°€í•œ ê²½ê³„ ì •ë³´ë¥¼ ë³µì›í•˜ê³ ì í•©ë‹ˆë‹¤.

**Edge attention seeks to exploit guided filter to extract edge detailed information from high-resolution images.**  
ì—£ì§€ ì–´í…ì…˜ì€ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œë¶€í„° ê²½ê³„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ê°€ì´ë“œ í•„í„°ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

**Hence, high-quality segmentation results can be obtained from low-resolution poorly segmented results and precise segmented boundaries can be maintained after up-sampling.**  
ë”°ë¼ì„œ, ë‚®ì€ í•´ìƒë„ì—ì„œë„ ì •ë°€í•œ ê²½ê³„ë¥¼ ìœ ì§€í•˜ë©° ê³ í’ˆì§ˆ ë¶„í•  ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Guided filter [38] is an edge-preserving image filter and has been incorporated into different deep learning tasks.**  
Guided filter [38]ëŠ” ê²½ê³„ë¥¼ ë³´ì¡´í•˜ëŠ” ì´ë¯¸ì§€ í•„í„°ì´ë©° ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ì‘ì—…ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.

---

**Assume that feature map $F_l$ with size $(c, h, w)$ and $F_h$ with size $(c, 2h, 2w)$ are given.**  
í¬ê¸°ê°€ $(c, h, w)$ì¸ $F_l$ê³¼ $(c, 2h, 2w)$ì¸ $F_h$ê°€ ì£¼ì–´ì§„ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.

**The edge attention first down-samples (bilinear) $F_h$ to the half of its scale to obtain $F_l$.**  
ì—£ì§€ ì–´í…ì…˜ì€ ë¨¼ì € $F_h$ë¥¼ ì´ì¤‘ì„ í˜• ë³´ê°„ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§í•˜ì—¬ $F_l$ì„ ìƒì„±í•©ë‹ˆë‹¤.

**Based on $F_l$ and $F_l$, the attention map $T$ can be obtained.**  
$F_l$ê³¼ $F_l$ì„ ê¸°ë°˜ìœ¼ë¡œ ì–´í…ì…˜ ë§µ $T$ê°€ ìƒì„±ë©ë‹ˆë‹¤.

**Then the reconstruction error is minimized to obtain the coefficients of the attention guided filter $w_k$ and $b_k$ for each square window $s_k$:**  
ì´í›„ ë‹¤ìŒ ì‹ì„ í†µí•´ ê° ì°½ $s_k$ì— ëŒ€í•œ í•„í„° ê³„ìˆ˜ $w_k$, $b_k$ë¥¼ ìµœì†Œ ì˜¤ì°¨ë¡œ ì¶”ì •í•©ë‹ˆë‹¤:

$$
F_{ki} = w_k F_{li} + b_k, \quad i \in s_k
$$

$$
\min_{w_k, b_k} E(w_k, b_k) = \sum_{i \in s_k} T_i^2(w_k F_{li} + b_k - F_{li})^2 + \lambda w_k^2
$$

**Then the output is expressed by linear combination:**  
ìµœì¢… ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ì¡°í•©ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤:

$$
F_i = W_l F_l + B_l
$$

**After upsampling $W_l$, $B_l$ to $W_h$, $B_h$, the final high-resolution output becomes:**  
$W_l$, $B_l$ì„ ì—…ìƒ˜í”Œë§í•˜ì—¬ $W_h$, $B_h$ë¥¼ ë§Œë“¤ê³ , ìµœì¢… ê³ í•´ìƒë„ ì¶œë ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
F_h = W_h F_h + B_h
$$

**Fig. 5 illustrates the edge attention process.**  
Fig. 5ëŠ” ì—£ì§€ ì–´í…ì…˜ ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤.  
â†’ ğŸ“Œ **[Figure 5 ì‚½ì… ìœ„ì¹˜]**

---

### **Double attention summary**

**The double attention module with channel attention and edge attention can guide the multi-scale network to extract more effective features for chicken segmentation.**  
ì±„ë„ ì–´í…ì…˜ê³¼ ì—£ì§€ ì–´í…ì…˜ìœ¼ë¡œ êµ¬ì„±ëœ ì´ì¤‘ ì–´í…ì…˜ ëª¨ë“ˆì€ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë„¤íŠ¸ì›Œí¬ê°€ ë” íš¨ê³¼ì ì¸ íŠ¹ì§•ì„ ì¶”ì¶œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

**The flowchart of the proposed double attention module based network is shown in Fig. 3.**  
ì´ì¤‘ ì–´í…ì…˜ì´ ì ìš©ëœ ì „ì²´ ë„¤íŠ¸ì›Œí¬ íë¦„ì€ Fig. 3ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.

---

### **Mask prediction and multi-scale side-outputs**

**The detailed pipeline of multi-scale side-outputs and combined loss are shown in Fig. 6.**  
ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‚¬ì´ë“œ ì¶œë ¥ê³¼ ê²°í•© ì†ì‹¤ì˜ ì„¸ë¶€ íë¦„ì€ Fig. 6ì— ë‚˜ì™€ ìˆìŠµë‹ˆë‹¤.  
â†’ ğŸ“Œ **[Figure 6 ì‚½ì… ìœ„ì¹˜]**

**After obtaining features $FEA_1$ to $FEA_n$ from edge attention and convolution operations, multi-scale outputs $M_1$ to $M_n$ are generated through the mask prediction block $MP()$ as follows:**  
ì—£ì§€ ì–´í…ì…˜ê³¼ í•©ì„±ê³± ì—°ì‚°ì„ ê±°ì¹œ $FEA_1$ë¶€í„° $FEA_n$ê¹Œì§€ì˜ íŠ¹ì§•ì€ ë§ˆìŠ¤í¬ ì˜ˆì¸¡ ë¸”ë¡ $MP()$ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì¶œë ¥ $M_1$ë¶€í„° $M_n$ê¹Œì§€ ìƒì„±ë©ë‹ˆë‹¤:

$$
M_1 = MP(FEA_1), \quad M_i = MP(FEA_i), \quad M_n = MP(FEA_n)
$$

**Here, $MP()$ includes up-sampling, convolution, and softmax operations.**  
ì—¬ê¸°ì„œ $MP()$ëŠ” ì—…ìƒ˜í”Œë§, í•©ì„±ê³±, ì†Œí”„íŠ¸ë§¥ìŠ¤ ì—°ì‚°ì„ í¬í•¨í•©ë‹ˆë‹¤.

**Each side-output serves as auxiliary supervision to help the network learn better segmentation.**  
ê° ì‚¬ì´ë“œ ì¶œë ¥ì€ ë³´ì¡° í•™ìŠµ ì‹ í˜¸ë¡œ ì‘ìš©í•˜ì—¬ ë” ë‚˜ì€ ë¶„í•  ê²°ê³¼ë¥¼ ë„ì¶œí•˜ë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.
